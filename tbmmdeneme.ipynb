{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "056a65c5-70b6-4fdf-a3c0-bd8d9fdf2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd9c3674-c6f0-44d9-8e83-f016ae121e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = r\"C:\\Users\\rnhhy\\turkish-parliament-texts\"\n",
    "sys.path.append(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "566fe6f0-7454-4ed2-ae57-c5694407201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corpus_loader\n",
    "import configparser\n",
    "from corpus_compiler import tbmmcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d83a378-7811-42a3-b276-d039a90abcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.corpora.textcorpus:No input document stream provided; assuming dictionary will be initialized some other way.\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 900 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 1000 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 1100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 1200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 1300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 1400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 1500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 1600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 1700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 1800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 1900 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 2000 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 2100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 2200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 2300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 2400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 2500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 2600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 2700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 2800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 2900 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 3000 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 3100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 3200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 3300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 3400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 3500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 3600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 3700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 3800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 3900 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 4000 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 4100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 4200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 4300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 4400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 4500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 4600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 4700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 4800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 4900 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 5000 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 5100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 5200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 5300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 5400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 5500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 5600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 5700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 5800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 5900 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 6000 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 6100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 6200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 6300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 6400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 6500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 6600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 6700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 6800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 6900 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 7000 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 7100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 7200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 7300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 7400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 7500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 7600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 7700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 7800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 7900 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 8000 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 8100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 8200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 8300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 8400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 8500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 8600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 8700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 8800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 8900 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 9000 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 9100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 9200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 9300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 9400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 9500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 9600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 9700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 9800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 9900 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 10000 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 10100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 10200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 10300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 10400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 10500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 10600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 10700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 10800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 10900 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 11000 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 11100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 11200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 11300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 11400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 11500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 11600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 11700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 11800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 11900 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 12000 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 12100 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 12200 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 12300 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 12400 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 12500 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 12600 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 12700 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 12800 documents\n",
      "INFO:corpus_compiler.tbmmcorpus:loaded 12900 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded successfully with inmemory=True\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "from corpus_compiler import tbmmcorpus\n",
    "import logging\n",
    "\n",
    "config_parser = configparser.ConfigParser()\n",
    "\n",
    "config_file_path = r\"C:\\Users\\rnhhy\\turkish-parliament-texts\\config.ini\" \n",
    "config_parser.read(config_file_path)\n",
    "\n",
    "if 'default' in config_parser:\n",
    "    config = config_parser['default']\n",
    "else:\n",
    "    raise KeyError(\"The 'default' section is not found in the configuration file.\")\n",
    "\n",
    "corpus_filepath = r\"C:\\Users\\rnhhy\\corpus-v0.4b\\tbmm_corpus.mm\"\n",
    "date_mappings_path = r\"C:\\Users\\rnhhy\\turkish-parliament-texts\\data\\date_mappings.pkl\"\n",
    "\n",
    "if not os.path.isfile(corpus_filepath):\n",
    "    raise FileNotFoundError(f\"The corpus file {corpus_filepath} does not exist.\")\n",
    "if not os.path.isfile(date_mappings_path):\n",
    "    raise FileNotFoundError(f\"The date mappings file {date_mappings_path} does not exist.\")\n",
    "\n",
    "corpus = tbmmcorpus.TbmmCorpus(metadata=True, config=config)\n",
    "\n",
    "corpus.load_tbmm_corpus(corpus_filepath, inmemory=True)\n",
    "\n",
    "print(\"Corpus loaded successfully with inmemory=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0f20f6b-80c5-4a92-a673-3962430ebad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from scipy import stats\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "326ce7da-772a-465c-ac92-badd2fc71d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35cad0a9-ce88-4f08-aa88-825ae631d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_in_batches(classifier, x_train, y_train, batch_size):\n",
    "    if hasattr(classifier, 'partial_fit'):\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            classifier.partial_fit(x_batch, y_batch, classes=np.unique(y_train))\n",
    "    else:\n",
    "        classifier.fit(x_train, y_train)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d2ba93a-0146-4076-938d-6fa8315f2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_steps(data, vectorizer, compressor, classifier):\n",
    "\n",
    "    fold_matrix = []\n",
    "    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    data_vector = vectorizer.fit_transform(data.data).astype('float32').toarray()\n",
    "\n",
    "    if compressor is not None:\n",
    "        if isinstance(compressor, LDA):\n",
    "            data_vector = compressor.fit_transform(data_vector, data.target)\n",
    "        else:\n",
    "            data_vector = compressor.fit_transform(data_vector)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(data_vector, data.target)):\n",
    "\n",
    "        data_train = data_vector[train_index]\n",
    "        data_test = data_vector[test_index]\n",
    "        data_train_target = data.target[train_index]\n",
    "        data_test_target = data.target[test_index]\n",
    "        data_train_target = np.array(data_train_target)\n",
    "        data_test_target = np.array(data_test_target)\n",
    "        \n",
    "        fitted_classifier = fit_in_batches(classifier, data_train, data_train_target, batch_size=500)\n",
    "        fold_matrix.append([data_train, data_test, data_train_target, data_test_target, fitted_classifier])\n",
    "   \n",
    "    return fold_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3370e0cb-0f30-4fea-81ae-8f55bdf51a42",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TbmmCorpus' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(classify, SVC):\n\u001b[0;32m     18\u001b[0m     classifier_instance \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m result \u001b[38;5;241m=\u001b[39m pre_steps(data, vec, comp, classifier_instance)\n\u001b[0;32m     21\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     22\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[32], line 5\u001b[0m, in \u001b[0;36mpre_steps\u001b[1;34m(data, vectorizer, compressor, classifier)\u001b[0m\n\u001b[0;32m      3\u001b[0m fold_matrix \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m kf \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m data_vector \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(data\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compressor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compressor, LDA):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TbmmCorpus' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "# Vectorizer and compressor combinations\n",
    "vectorizer = [CountVectorizer(), TfidfVectorizer()]\n",
    "compressor = [None, IncrementalPCA(n_components=50, batch_size=1000), LDA(n_components=2)]\n",
    "classifier = [SGDClassifier(loss=\"hinge\", penalty=\"l2\", alpha=1e-3, random_state=42, max_iter=5, tol=None), LogisticRegression(max_iter=1000), \n",
    "              SVC(kernel='linear', random_state=42)]\n",
    "combinations = [(vec, comp, classify) for vec in vectorizer for comp in compressor for classify in classifier]\n",
    "\n",
    "# Re-initialize classifier for each combination\n",
    "all_results = []\n",
    "execution_times = []\n",
    "for vec, comp, classify in combinations:\n",
    "    start_time = time.time()\n",
    "    if isinstance(classify, SGDClassifier):\n",
    "        classifier_instance = SGDClassifier(loss=\"hinge\", penalty=\"l2\", alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "    elif isinstance(classify, LogisticRegression):\n",
    "        classifier_instance = LogisticRegression(max_iter=1000)\n",
    "    elif isinstance(classify, SVC):\n",
    "        classifier_instance = SVC(kernel='linear', random_state=42)\n",
    "    \n",
    "    result = pre_steps(data, vec, comp, classifier_instance)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    execution_times.append(execution_time)\n",
    "    print(f\"Evaluating Model with {vec.__class__.__name__}, {comp.__class__.__name__ if comp else 'None'}, {classify.__class__.__name__}: {execution_time: .6f} seconds\")\n",
    "    all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89934483-879d-48b6-a657-8ec2921fe8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
