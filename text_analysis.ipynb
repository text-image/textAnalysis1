{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6d58b429-0478-4d8f-97e6-b9bae0f21659",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from scipy import stats\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dbada454-c7df-4bf7-bf66-032ad192889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ["alt.atheism", "sci.med", "talk.politics.mideast", "rec.motorcycles"]",
    "data = load_files(container_path='C:\\\\20_newsgroups', categories=categories, description=None, load_content=True,\n",
    "                         shuffle=True, encoding='latin-1', decode_error='strict', random_state=42,\n",
    "                         allowed_extensions=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0eb884a2-0593-4916-a3d1-4a674d975eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_in_batches(classifier, x_train, y_train, batch_size):\n",
    "    if hasattr(classifier, 'partial_fit'):\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            classifier.partial_fit(x_batch, y_batch, classes=np.unique(y_train))\n",
    "    else:\n",
    "        classifier.fit(x_train, y_train)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "65c1076a-2f2a-4a5b-a0df-8383bb39cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_steps(data, vectorizer, compressor, classifier):\n",
    "\n",
    "    fold_matrix = []\n",
    "    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    data_vector = vectorizer.fit_transform(data.data).astype('float32').toarray()\n",
    "\n",
    "    if compressor is not None:\n",
    "        if isinstance(compressor, LDA):\n",
    "            data_vector = compressor.fit_transform(data_vector, data.target)\n",
    "        else:\n",
    "            data_vector = compressor.fit_transform(data_vector)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(data_vector, data.target)):\n",
    "\n",
    "        data_train = data_vector[train_index]\n",
    "        data_test = data_vector[test_index]\n",
    "        data_train_target = data.target[train_index]\n",
    "        data_test_target = data.target[test_index]\n",
    "        data_train_target = np.array(data_train_target)\n",
    "        data_test_target = np.array(data_test_target)\n",
    "        \n",
    "        fitted_classifier = fit_in_batches(classifier, data_train, data_train_target, batch_size=500)\n",
    "        fold_matrix.append([data_train, data_test, data_train_target, data_test_target, fitted_classifier])\n",
    "   \n",
    "    return fold_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "92a4bf25-7137-4e46-ab40-799df50ab57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model with CountVectorizer, None, SGDClassifier:  9.736298 seconds\n",
      "Evaluating Model with CountVectorizer, None, LogisticRegression:  110.129589 seconds\n",
      "Evaluating Model with CountVectorizer, None, SVC:  295.732613 seconds\n",
      "Evaluating Model with CountVectorizer, IncrementalPCA, SGDClassifier:  19.839501 seconds\n",
      "Evaluating Model with CountVectorizer, IncrementalPCA, LogisticRegression:  19.246647 seconds\n",
      "Evaluating Model with CountVectorizer, IncrementalPCA, SVC:  18.550719 seconds\n",
      "Evaluating Model with CountVectorizer, LinearDiscriminantAnalysis, SGDClassifier:  94.599482 seconds\n",
      "Evaluating Model with CountVectorizer, LinearDiscriminantAnalysis, LogisticRegression:  102.789285 seconds\n",
      "Evaluating Model with CountVectorizer, LinearDiscriminantAnalysis, SVC:  105.076224 seconds\n",
      "Evaluating Model with TfidfVectorizer, None, SGDClassifier:  12.779468 seconds\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 525. MiB for an array with shape (2700, 50968) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(classify, SVC):\n\u001b[0;32m     18\u001b[0m     classifier_instance \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m result \u001b[38;5;241m=\u001b[39m pre_steps(twenty_news, vec, comp, classifier_instance)\n\u001b[0;32m     21\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     22\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[126], line 15\u001b[0m, in \u001b[0;36mpre_steps\u001b[1;34m(data, vectorizer, compressor, classifier)\u001b[0m\n\u001b[0;32m     11\u001b[0m         data_vector \u001b[38;5;241m=\u001b[39m compressor\u001b[38;5;241m.\u001b[39mfit_transform(data_vector)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (train_index, test_index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(kf\u001b[38;5;241m.\u001b[39msplit(data_vector, data\u001b[38;5;241m.\u001b[39mtarget)):\n\u001b[1;32m---> 15\u001b[0m     data_train \u001b[38;5;241m=\u001b[39m data_vector[train_index]\n\u001b[0;32m     16\u001b[0m     data_test \u001b[38;5;241m=\u001b[39m data_vector[test_index]\n\u001b[0;32m     17\u001b[0m     data_train_target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtarget[train_index]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 525. MiB for an array with shape (2700, 50968) and data type float32"
     ]
    }
   ],
   "source": [
    "# Vectorizer and compressor combinations\n",
    "vectorizer = [CountVectorizer(), TfidfVectorizer()]\n",
    "compressor = [None, IncrementalPCA(n_components=50, batch_size=1000), LDA(n_components=2)]\n",
    "classifier = [SGDClassifier(loss=\"hinge\", penalty=\"l2\", alpha=1e-3, random_state=42, max_iter=5, tol=None), LogisticRegression(max_iter=1000), \n",
    "              SVC(kernel='linear', random_state=42)]\n",
    "combinations = [(vec, comp, classify) for vec in vectorizer for comp in compressor for classify in classifier]\n",
    "\n",
    "# Re-initialize classifier for each combination\n",
    "all_results = []\n",
    "execution_times = []\n",
    "for vec, comp, classify in combinations:\n",
    "    start_time = time.time()\n",
    "    if isinstance(classify, SGDClassifier):\n",
    "        classifier_instance = SGDClassifier(loss=\"hinge\", penalty=\"l2\", alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "    elif isinstance(classify, LogisticRegression):\n",
    "        classifier_instance = LogisticRegression(max_iter=1000)\n",
    "    elif isinstance(classify, SVC):\n",
    "        classifier_instance = SVC(kernel='linear', random_state=42)\n",
    "    \n",
    "    result = pre_steps(data, vec, comp, classifier_instance)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    execution_times.append(execution_time)\n",
    "    print(f\"Evaluating Model with {vec.__class__.__name__}, {comp.__class__.__name__ if comp else 'None'}, {classify.__class__.__name__}: {execution_time: .6f} seconds\")\n",
    "    all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c0120300-f0f2-4f7d-96c3-345f4e6ba83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model 1 with CountVectorizer, None, SGDClassifier:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'execution_times' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m     mean_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(model_scores)\n\u001b[0;32m     16\u001b[0m     confidence_interval \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mt\u001b[38;5;241m.\u001b[39minterval(\u001b[38;5;241m0.95\u001b[39m, \u001b[38;5;28mlen\u001b[39m(model_scores)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, loc\u001b[38;5;241m=\u001b[39mmean_score, scale\u001b[38;5;241m=\u001b[39mstats\u001b[38;5;241m.\u001b[39msem(model_scores))\n\u001b[0;32m     17\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvec\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mcomp\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassify\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m---> 19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution Time (s)\u001b[39m\u001b[38;5;124m\"\u001b[39m: execution_times[i],\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross-Validation Score\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39maverage(model_scores),\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Score\u001b[39m\u001b[38;5;124m\"\u001b[39m: mean_score,\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfidence Interval\u001b[39m\u001b[38;5;124m\"\u001b[39m: confidence_interval\n\u001b[0;32m     23\u001b[0m         \n\u001b[0;32m     24\u001b[0m     })\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(results)    \n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#    print(f\"Average of the Cross-Validation Scores: {', '.join(f'{np.average(model_scores):.4f}')}\")\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'execution_times' is not defined"
     ]
    }
   ],
   "source": [
    "# EVALUATE\n",
    "all_scores = []\n",
    "results = []\n",
    "\n",
    "for i, result in enumerate(all_results):\n",
    "    vec, comp, classify = combinations[i]\n",
    "    print(f\"Evaluating Model {i + 1} with {vec.__class__.__name__}, {comp.__class__.__name__ if comp else 'None'}, {classify.__class__.__name__}:\")\n",
    "    model_scores = []\n",
    "    for j, fold in enumerate(result):\n",
    "        data_train, data_test, data_train_target, data_test_target, fitted_classifier = fold\n",
    "        fold_scores = cross_val_score(fitted_classifier, data_train, data_train_target, cv=5, scoring='accuracy')\n",
    "        model_scores.extend(fold_scores)\n",
    "        all_scores.extend(model_scores)\n",
    "\n",
    "    mean_score = np.mean(model_scores)\n",
    "    confidence_interval = stats.t.interval(0.95, len(model_scores)-1, loc=mean_score, scale=stats.sem(model_scores))\n",
    "    results.append({\n",
    "        \"Model\": f\"{vec.__class__.__name__}, {comp.__class__.__name__ if comp else 'None'}, {classify.__class__.__name__}\",\n",
    "        \"Execution Time (s)\": execution_times[i],\n",
    "        \"Cross-Validation Score\": np.average(model_scores),\n",
    "        \"Mean Score\": mean_score,\n",
    "        \"Confidence Interval\": confidence_interval\n",
    "        \n",
    "    })\n",
    "    print(results)    \n",
    "#    print(f\"Average of the Cross-Validation Scores: {', '.join(f'{np.average(model_scores):.4f}')}\")\n",
    "    \n",
    "mean_score = np.mean(all_scores)\n",
    "confidence_interval = stats.t.interval(0.95, len(all_scores)-1, loc=mean_score, scale=stats.sem(all_scores))\n",
    "print(f\"Mean score: {mean_score}\\n%95 Confidence Interval: {confidence_interval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6479b-8d05-42e5-b15b-4d630f407f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA FRAME\n",
    "df_results = pd.DataFrame(results)\n",
    "data = [[\"Mean Score\", \"Confidence Interval\"], [mean_score, confidence_interval]]\n",
    "\n",
    "# EXCEL\n",
    "df_results.to_excel('model_evaluation.xlsx', index=False)\n",
    "\n",
    "print(\"'model_evaluation.xlsx' is created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e64da7-99a7-4cab-b02b-d0bbff9f484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD NEW DATA\n",
    "file_name = 'model_evaluation.xlsx'\n",
    "\n",
    "if os.path.exists(file_name):\n",
    "    existing_df = pd.read_excel(file_name)\n",
    "    df_results = pd.concat([existing_df, df_results], ignore_index=True)\n",
    "\n",
    "df_results.to_excel(file_name, index=False)\n",
    "\n",
    "print(\"'model_evaluation.xlsx' is updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
